<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-11-26T13:21:05+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Jin’s Research Blog</title><subtitle>Keep it simple and easy</subtitle><author><name>Jin Wang</name><email>jinwang1904@gmail.com</email></author><entry><title type="html">Learning-driven task offloading in multi-access edge computing</title><link href="http://localhost:4000/2020/12/17/rl-mec.html" rel="alternate" type="text/html" title="Learning-driven task offloading in multi-access edge computing" /><published>2020-12-17T00:09:00+08:00</published><updated>2020-12-17T00:09:00+08:00</updated><id>http://localhost:4000/2020/12/17/rl-mec</id><content type="html" xml:base="http://localhost:4000/2020/12/17/rl-mec.html">&lt;blockquote&gt;
  &lt;p&gt;Multi-access edge computing (MEC) has attracted lots of research interests from both academic and industrial fields, which aims to extend cloud service to the network edge to reduce network traffic and service latency. One key decision-making problem in MEC systems is task offloading which aims to decide which tasks should be migrated to the MEC server to minimize the running cost. In this article, I am going to talk about how to use learning-based methods to address the above problem. 
&lt;!--more--&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;an-overview-of-multi-access-edge-computing&quot;&gt;An overview of Multi-access edge computing&lt;/h2&gt;

&lt;p&gt;The technological evolution of smartphones, tablets, and wearable devices is driving the emergence of novel computationally demanding services and applications such as virtual reality (VR), augmented reality (AR), face recognition, and mobile healthcare. Although new generations of mobile devices possess more computing power, they are still unable to run the computation-intensive applications efficiently. To resolve this issue, &lt;a href=&quot;https://www.etsi.org/technologies/multi-access-edge-computing&quot;&gt;Multi-access Edge Computing (MEC)&lt;/a&gt;, a key technology in the fifth-generation (5G) network, was proposed to meet the ever-increasing demands for the Quality-of-Service (QoS) of mobile applications. MEC provides many computing and storage resources at the network edge (close to users), which can effectively cut down the application latency and improve the QoS.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/MEC-system-applications.png&quot; alt=&quot;MEC-system application&quot; /&gt;
&lt;em&gt;&lt;center&gt;Fig. 1 MEC system architecture.&lt;/center&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Numerous MEC applications have emerged in different areas, which improve the quality of living and enhance productivity. Meanwhile, a variety of applications such as social networking, gaming, and AR become increasingly complex, and therefore demand more computing resources and energy. In the definition of the ESTI, a MEC system generally consists of tree levels: user level, edge level, and remote level. Here, the user level includes heterogeneous user devices, the edge level contains MEC servers that provide edge computing services, and the remote level consists of cloud servers. Specifically, mobile users communicate with an MEC server through the local transmission unit. The edge level incorporates a virtualization infrastructure that provides the computing, storage, and network resources.&lt;/p&gt;

&lt;h3 id=&quot;task-offloading-in-mec&quot;&gt;Task offloading in MEC&lt;/h3&gt;

&lt;p&gt;Task offloading is to decision what tasks should be offloaded from the user devices to the MEC server so that the total running cost is minimal. In general, there are two task models used in the related work: task model for binary offloading and that for partial offloading according to a &lt;a href=&quot;https://ieeexplore.ieee.org/document/8016573&quot;&gt;survey paper&lt;/a&gt;. In the task model for binary offloading, there are no inner dependencies among computation tasks for a mobile application. In contrast, mobile applications are composed of tasks with inner dependencies in the task model for partial offloading, which is able to achieve a fine granularity of computation offloading, leading to better offloading performance. Hence, we consider task model for partial offloading.&lt;br /&gt;
&lt;img src=&quot;/assets/images/mec-example.png&quot; alt=&quot;example&quot; width=&quot;500&quot; /&gt;
&lt;em&gt;&lt;center&gt;Fig. 2 An example of task offloading.&lt;/center&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The above figure shows a typical example of the task offloading in MEC system. This example considers a real-world application—face
recognition, which consists of dependent tasks such as tiler, detection, or feature mergence. The mobile device makes offloading decisions for those tasks according to the system status and task profiles, thus some tasks are run locally on the mobile device while others are offloaded to the MEC host via wireless channels. Typically, the mobile application can be modelled as Directed Acyclic Graph (DAG), where the vertex set represents the tasks and the directed edge set represents the dependencies among tasks, respectively. The task offloading problem can then be converted to specific DAG scheduling problem with the MEC environment. However, this problem is hard to solve due to the NP-hardness. Traditional solutions for the NP-hard problem requires considerable human expert knowledge to design and tuning the heuristic rules of the algorithm, which might hard to fast adapt to dynamic scenarios arisen from increasing complexity of applications and architectures of MEC. To address the above challenging, &lt;a href=&quot;http://incompleteideas.net/book/the-book.html&quot;&gt;Reinforcement learning&lt;/a&gt; based algorithms are promising which learn to solve complex decision-making problems based on the interaction with the target environment.&lt;/p&gt;

&lt;h2 id=&quot;reinforcement-learning-based-task-offloading-in-mec&quot;&gt;Reinforcement Learning based task offloading in MEC&lt;/h2&gt;

&lt;p&gt;In order to design reinforcement learning based task offloading method, we need to tailer the components of the RL including state space, action space, reward function, representation of the policy, etc. according to the task offloading problem. Let us first review basic concept of reinforcement learning.&lt;/p&gt;

&lt;h3 id=&quot;a-brief-introduction-of-reinforcement-learning&quot;&gt;A Brief Introduction of Reinforcement Learning&lt;/h3&gt;

&lt;p&gt;RL considers learning from environment so as to maximize the accumulated reward. Formally, a learning task,  $\mathcal{T}$, is modelled as an MDP, which is defined by a tuple $( \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{P}_0, \mathcal{R}, \gamma )$. Here, $\mathcal{S}$ is the state space, $\mathcal{A}$ denotes the action space, $\mathcal{R}$ is a reward function, $\mathcal{P}$ is the state-transition probabilities matrix, $\mathcal{P}_0$ is the initial state distribution, and $\gamma \in [0, 1]$ is the discount factor. A policy $\pi(a \mid s)$, where $a \in \mathcal{A}$ and $s \in \mathcal{S}$, is a mapping from state $s$ to the probability of selecting action $a$. We define the trajectories sampled from the environment according to the policy $\pi$ as $\tau = (s_0, a_0, r_0, s_1, a_1, r_1, …)$, where $s_0 \sim \mathcal{P}_0$, $a_t \sim \pi( … \mid s_t)$ and $r_t$ is a reward at time step $t$.&lt;/p&gt;

&lt;p&gt;The state value function of a state $s_t$ under a parameterized policy $\pi(a \mid s; \theta)$,  denoted as $v_{\pi}(s_t)$, is the expected return when starting in $s_t$ and following $\pi(a \mid s; \theta)$ thereafter. Here, $\theta$ is the vector of policy parameters and $v_{\pi}(s_t)$ can be calculated by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;v_{\pi}(s_t)=\mathbb{E}_{\tau \sim P_{\mathcal{T}}(\tau \mid \theta)}\left[ \sum_{k=t}\gamma^{k-t} r_k \right]&lt;/script&gt;

&lt;p&gt;where $P_{\mathcal{T}}(\tau \mid \theta)$ is the probability distribution of sampled trajectories based on $\pi(a \mid s; \theta)$. The goal for RL is to find an optimal parameterized policy $\pi(a \mid s; \theta^*)$ to maximize the expected total rewards $ J = \sum_{s_0 \sim \mathcal{P}_0} v(s_0)$.&lt;/p&gt;

&lt;h3 id=&quot;reinforcement-learning-based-task-offloading&quot;&gt;Reinforcement Learning based Task offloading&lt;/h3&gt;

&lt;p&gt;The above task offloading problem is a typical combinatorial optimization problem, inspired by some early works that use sequence-to-sequence (seq2seq) neural network to solve combinatorial problem (&lt;a href=&quot;https://arxiv.org/abs/1506.03134&quot;&gt;pointer network&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1611.09940&quot;&gt;RL+pointer network&lt;/a&gt;), our core idea is to convert the task offloading into a sequence-to-sequence decision-making problem, where the input is a sequence of tasks and the output is the decisions of the tasks. The offloading policy is represented by a seq2seq neural network as follows:
&lt;img src=&quot;/assets/images/seq2seq-network.png&quot; alt=&quot;example&quot; width=&quot;500&quot; /&gt;
&lt;em&gt;&lt;center&gt;Fig. 3 The structure of sequence to sequence neural network.&lt;/center&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The policy network is a typical encoder-decoder architecture, where both encoder and decoder are Long Short-Term Memory (LSTM). The encoder aims encoder the DAG information and decoder aims to output offloading decisions for tasks. To better extract features from the DAG, the encoder can be replaced by &lt;a href=&quot;https://arxiv.org/abs/1901.00596&quot;&gt;Graph Neural Network (GNN)&lt;/a&gt; which shows good representing performance handling graph data.&lt;/p&gt;

&lt;p&gt;After we choose the right model to represent the policy, the next issue we need to solve is how to train the policy network in MEC system? Considering different mobile users may have different preference of mobile applications, it is better to obtain personlized policy for mobile users rather than obtaining a global policy for all users. Besides, considering the limited computation resources and local data of mobile users, we need to improve the training efficiency on mobile devices. To address the above issue, &lt;a href=&quot;https://sites.google.com/view/icml19metalearning&quot;&gt;Meta Reinforcement Learning (MetaRL)&lt;/a&gt; is an appealing solution.&lt;/p&gt;

&lt;p&gt;MetaRL aims to learn policies for new tasks within a small number of interactions with the environment by building on previous experiences. In general, MRL conducts two “loops” of learning, an “outer loop” which uses its experiences over many task contexts to gradually adjust parameters of the meta policy that governs the operation of an “inner loop”. Based on the meta policy, the “inner loop” can adapt fast to new tasks through a small number of gradient updates (here is a &lt;a href=&quot;https://www.cell.com/action/showPdf?pii=S1364-6613%2819%2930061-0&quot;&gt;very good paper&lt;/a&gt; to describe MRL). To make it concrete, we follow the formulation of model-agnostic meta-learning (MAML), giving the target of MRL as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(\theta) = \mathbb{E}_{\mathcal{T}_i \sim \rho(\mathcal{T})} \left [  J_{\mathcal{T}_i}(\theta')  \right ], {\rm with} \ \theta' := U(\theta, \mathcal{T}_i),&lt;/script&gt;

&lt;p&gt;where $\mathcal{T}$ represents a learning task, $\rho(\mathcal{T})$ represents the distribution of learning tasks, and $\theta$ represent the parameters of the policy network. $U$ represents the update function, for instance, if we conduct $k$-step gradient ascent for $\mathcal{T}$, then 
$U(\theta, \mathcal{T}) = \theta + \alpha\sum_{t=1}^k g_t$, where $g_t$ denotes the gradient of $J_{\mathcal{T}_i}$ at time step $t$ and $\alpha$ is the learning rate.&lt;/p&gt;

&lt;p&gt;Therefore, the optimal parameters of policy network and update rules are&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\label{MRL-update}
\begin{aligned}
&amp; \theta^{*} = {\rm argmax}_{\theta} \mathbb{E}_{\mathcal{T}_i \sim \rho(\mathcal{T})} \left [  J_{\mathcal{T}_i}(U(\theta, \mathcal{T}_i)  \right ], \\
&amp;\theta \leftarrow \theta + \beta \mathbb{E}_{\mathcal{T}_i \sim \rho(\mathcal{T})} \left [ \nabla_{\theta}J_{\mathcal{T}_i}(U(\theta, \mathcal{T}_i)  \right ],
\end{aligned}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $\beta$ is the learning rate of “outer loop” training.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/meta-rl-offloading.png&quot; alt=&quot;example&quot; width=&quot;700&quot; /&gt;
&lt;em&gt;&lt;center&gt;Fig. 4 Integrating meta reinforcement learning with MEC.&lt;/center&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This “inner-outer loop” training scheme can be perfectly integrated into the MEC system, where the “outer loop” is conducted on the edge server to obtain the meta offloading policy for all mobile users and the “inner loop” training can be conducted by the mobile user to get the personalized offloading policy. As shown in Fig. 4, the training process includes four steps. First, the mobile user downloads the parameters of the meta policy from the MEC server. Next, an “inner loop” training is run on every mobile device based on the meta policy and the local data, in order to obtain the personalized policy. The mobile user then uploads the parameters of the personalized policy to the MEC server. Finally, the MEC server conducts an “outer loop” training based on the gathered parameters of personalized policies, generates the new meta policy, and starts a new round of training. Once obtaining the stable meta policy, we can leverage it to fast learn a personalized policy for new mobile user through “inner loop” training. Notice that the “inner loop” training only needs few training steps and a small amount of data, thus can be sufficiently supported by the mobile devices.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;This post is based on the following papers:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://ore.exeter.ac.uk/repository/bitstream/handle/10871/36902/confpaper.pdf;jsessionid=61B0B4362F3AFFAF5968729F1F23B01D?sequence=1&quot;&gt;Computation Offloading in Multi-Access Edge Computing Using a Deep Sequential Model Based on Reinforcement Learning&lt;/a&gt;. IEEE Communications Magazine, vol. 57, no. 5, pp. 64–69, 2019.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2008.02033&quot;&gt;Fast Adaptive Computation Offloading in Edge Computing based on Meta Reinforcement Learning&lt;/a&gt;. IEEE Transactions on Parallel and Distributed Systems, vol. 32, no. 1, pp. 242–253, 2020.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Jin Wang</name><email>jinwang1904@gmail.com</email></author><category term="MEC" /><category term="task-offloading" /><category term="service-migration" /><category term="reinforcement-learning" /><summary type="html">Multi-access edge computing (MEC) has attracted lots of research interests from both academic and industrial fields, which aims to extend cloud service to the network edge to reduce network traffic and service latency. One key decision-making problem in MEC systems is task offloading which aims to decide which tasks should be migrated to the MEC server to minimize the running cost. In this article, I am going to talk about how to use learning-based methods to address the above problem.</summary></entry><entry><title type="html">From policy gradient theorem to variational autoencoder</title><link href="http://localhost:4000/2020/03/22/pg-vae.html" rel="alternate" type="text/html" title="From policy gradient theorem to variational autoencoder" /><published>2020-03-22T00:09:00+08:00</published><updated>2020-03-22T00:09:00+08:00</updated><id>http://localhost:4000/2020/03/22/pg-vae</id><content type="html" xml:base="http://localhost:4000/2020/03/22/pg-vae.html">&lt;blockquote&gt;
  &lt;p&gt;Policy gradient theorem is the basis of many deep reinforcement learning methods (e.g., &lt;a href=&quot;https://arxiv.org/abs/1707.06347&quot;&gt;PPO&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1502.05477&quot;&gt;TRPO&lt;/a&gt;, and &lt;a href=&quot;https://arxiv.org/abs/1801.01290&quot;&gt;SAC&lt;/a&gt;). Meanwhile, Variational Auto-encoder (VAE) is one of the popular generative methods in machine learning. Since the target problems of these approaches share the same formulation, we can use policy gradient as an alternative training method for VAE.
&lt;!--more--&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;what-is-policy-gradient-theorem&quot;&gt;What is policy gradient theorem&lt;/h2&gt;

&lt;p&gt;Policy gradient theorem is widely applied to solve RL problems. The theoretical model that reinforcement learning (RL) tries to handle is Markov Decision Process (MDP). In general, MDP consists of a six-element tuple $\mathcal{T} = &amp;lt;\mathcal{S}, \mathcal{A}, \mathcal{R}, \mathcal{P},  \mathcal{P}_0, \gamma&amp;gt;$. Here, $\mathcal{S}$ is the state space, $\mathcal{A}$ denotes the action space, $\mathcal{R}$ is a reward function, $\mathcal{P}$ is the state-transition probabilities matrix, $\mathcal{P}_0$ is the initial state distribution, and
$\gamma \in [0, 1]$ is the discount factor. A policy $\pi(a | s)$, where $a \in \mathcal{A}$ and $s \in \mathcal{S}$, is a mapping from state $s$ to the probability of selecting action $a$. A trajectory $\tau$ is defined by a sequence of sampling history under policy&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\tau_{\pi}=(s_0, a_0, r_0, s_1, a_1, r_1, ...).&lt;/script&gt;

&lt;p&gt;The state value function of a state $s_t$ under a policy $\pi$ is denoted as $v_\pi(s_t)$, which is the expected return when starting in $s_t$ and following $\pi$ thereafter. Thus the value function can be calculated as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;v_{\pi}(s_t) = \mathbb{E}_{\tau \sim P_{\mathcal{T}}(\tau|\pi)} \left [ \sum_{k=t} \gamma^{k-t} r_k \right ].&lt;/script&gt;

&lt;p&gt;In literature, researchers tend to use state-action value function, denoted as $q_\pi (s_t, a_t)$, which can be derived by state value function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;q_\pi(s_t, a_t) = \mathbb{E}_{a_t \sim \pi (a_t |s_t) }v_{\pi}(s_t ).&lt;/script&gt;

&lt;p&gt;The goal of reinforcment learning is to obtain the optimal policy $\pi^* $ to maximize the expected total reward:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J = \mathbb{E}_{s_0 \sim \mathcal{P}_0} \left [ v_{\pi}(s_0) \right ] =  \mathbb{E}_{\tau \sim \pi, \mathcal{S}}\left [ \sum_{t=0}^{T} \gamma^t q_{\pi}(s_t, a_t) \right ] .&lt;/script&gt;

&lt;p&gt;This can be seen as a typical numerical optimisation problem, where the policy is a parametrised by $\theta$. The key to find the optimal policy is to calculate the gradient of the target function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla_{\theta}J = \nabla_{\theta} \mathbb{E}_{s_0 \sim \mathcal{P}_0} \mathbb{E}_{a_0 \sim \pi(a_0|s_0)} q_\pi(s_0, a_0) = \mathbb{E}_{s_0 \sim \mathcal{P}_0} \nabla_{\theta} \mathbb{E}_{a_0 \sim \pi(a_0|s_0)} q_\pi(s_0, a_0)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{split}
 \nabla_{\theta} \mathbb{E}_{a \sim \pi(a|s)} q_\pi(s_0, a_0) &amp;=
   \sum_{s} \left (\sum_{k=0}^{\infty} Pr(s_0 \rightarrow s, k, \pi) \right)  \sum_{a}\nabla_{\theta}\pi(a|s)q_{\pi}(s, a), \\
   &amp;\propto \sum_{s}\mu(s) \sum_a \nabla_{\theta} \pi(a|s)q_\pi(s, a) \\
   &amp;= \sum_{s}\mu(s) \sum_a \frac{\nabla_{\theta}\pi(a|s)}{\pi(a|s)}\pi(a|s)q_\pi(s, a) \\
   &amp;= \sum_{s}\mu(s) \sum_a  \pi(a|s) q_\pi(s,a) \nabla_{\theta}\log\pi(a|s). \\

\end{split}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;Here we apply the Monte-Carlo method  (refer to &lt;a href=&quot;http://incompleteideas.net/book/RLbook2018.pdf&quot;&gt;reinforcement learning: an introduction&lt;/a&gt; chapter 5  ) to obtain the graident through the sampling trajectories $\tau = (s_0,a_0,r_0, …, s_T, a_T, r_T)$, thus we have:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{split}
\sum_{s}\mu(s) \sum_a  \pi(a|s) q_\pi(s,a) \nabla_{\theta}\log\pi(a|s) &amp;\approx \mathbb{E}_{\tau \sim \pi, \mathcal{P}} \left [ \sum_{t=0}^{T} \gamma^t \nabla_{\theta}\log\pi(a_t|s_t) q_\pi(s_t,a_t) \right ] \\
\end{split}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;The above equation is the basis of the policy gradient methods such as &lt;a href=&quot;https://arxiv.org/abs/1707.06347&quot;&gt;PPO&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1502.05477&quot;&gt;TRPO&lt;/a&gt;, and &lt;a href=&quot;https://arxiv.org/abs/1801.01290&quot;&gt;SAC&lt;/a&gt;. A simple approximation to approximate the policy gradient is to use the total reward for each trajectory instead of using q-values as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla_{\theta}J = \mathbb{E}_{\tau \sim \pi, \mathcal{P}} \left [ \left ( G_{\tau} - b(s) \right ) \nabla_{\theta}\log \pi(a_t | s_t)\right ],&lt;/script&gt;

&lt;p&gt;where $b(s)$ is a baseline that does not vary with action $a$.&lt;/p&gt;

&lt;h2 id=&quot;what-is-the-vae&quot;&gt;What is the VAE&lt;/h2&gt;
&lt;p&gt;VAE is a generative model based on latent variable. The fundamental mathematic problem that a generative model needs to solve is to obtain the distribution, $p(x)$, over the original data in some potentially high-dimensional space, assuming $p(x)$ is parametrised by $\theta$, where $p(x) \approx p_{\theta}(x)$. However, obtaining this distribution can be challenging.  Lots of methods get the optimal parameter $\theta$ by &lt;code class=&quot;highlighter-rouge&quot;&gt;maximum likelihood fit&lt;/code&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta \leftarrow {\rm argmax}_{\theta} \frac{1}{N} \sum_i^N \log p_{\theta} (x_i)&lt;/script&gt;

&lt;p&gt;Current methods that directly calculate $p(x)$ have had one of tree drawbacks: 1) they might require strong assumptions about the structure in the data; 2) they might make severe approximations, leading to suboptimal models; 3) they might rely on computationally expensive inference procedures like Markov Chain Monte Carlo.&lt;/p&gt;

&lt;p&gt;VAE solves this problem by using &lt;em&gt;latent model&lt;/em&gt;. Instead of directly calculate $p(x)$, the objective of VAE is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x) = \int_{z} p(x|z)p(z),&lt;/script&gt;

&lt;p&gt;where &lt;code class=&quot;highlighter-rouge&quot;&gt;$p(z)$&lt;/code&gt; is a simple distribution (e.g., gaussian distribution) and &lt;code class=&quot;highlighter-rouge&quot;&gt;$p(x|z)$&lt;/code&gt; can be approximated by parametrised functions (e.g., neural networks). Then, we can calculate the parameters by using maximum likelihood as：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta \leftarrow {\rm argmax}_{\theta} \frac{1}{N} \sum_i^N \log \left( \int_{z} p_{\theta}(x|z)p(z) d_z \right ),&lt;/script&gt;

&lt;p&gt;However, calculate the term &lt;code class=&quot;highlighter-rouge&quot;&gt;$ \int_{z} p_{\theta}(x|z)p(z) d_z$&lt;/code&gt; is intractable, which might need extremely large number of samples of $z$ in high dimensional spaces.&lt;/p&gt;

&lt;p&gt;VAE solves this problem by assuming that the sampled values of z are likely to have produced x, and computes &lt;code class=&quot;highlighter-rouge&quot;&gt;p(x)&lt;/code&gt; just from those sampled values. This means that we need a new function &lt;code class=&quot;highlighter-rouge&quot;&gt;$p(z|x)$&lt;/code&gt; which can take a value of $x$ and give the distribution over z values that are likely to produce x. Hopefully, the space of $z$ values that are likely under &lt;code class=&quot;highlighter-rouge&quot;&gt;$p(z|x)$&lt;/code&gt; will be much smaller than the space of all z’s that are likely under the prior $p(z)$. However, it is still hard to calculate &lt;code class=&quot;highlighter-rouge&quot;&gt;$p(z|x)$&lt;/code&gt;. VAE uses &lt;a href=&quot;https://arxiv.org/abs/1601.0067&quot;&gt;variational inference&lt;/a&gt; to approximate &lt;code class=&quot;highlighter-rouge&quot;&gt;p(z|x)$&lt;/code&gt; as &lt;code class=&quot;highlighter-rouge&quot;&gt;$q(z|x; \theta) \approx p(z|x)$&lt;/code&gt;, where &lt;code class=&quot;highlighter-rouge&quot;&gt;$q(z|x; \theta)$&lt;/code&gt; is a parameterized distribution. In order to minimise the distance between &lt;code class=&quot;highlighter-rouge&quot;&gt;$q(z|x)$&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;p(z|x)&lt;/code&gt;, we introduce the Kullback-Leibler divergence (KL-divergence) which is defined as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{\rm KL}(q(z|x)\|p(z|x)) = \mathbb{E}_{z \sim q(z|x)} \left [ \log \frac{q(z|x)}{p(z|x)} \right ].&lt;/script&gt;

&lt;p&gt;By applying Bayes rule to &lt;code class=&quot;highlighter-rouge&quot;&gt;$p(z|x_i)$&lt;/code&gt;, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{split}
  {\rm KL}(q(z|x)\|p(z|x)) &amp;=\mathbb{E}_{z \sim q(z|x)} \left [ \log \frac{q(z|x)p(x)}{p(x|z)p(z)} \right ] \\
  &amp;= \mathbb{E}_{z \sim q(z|x)} \left [ \log q(z|x) \right ] + \mathbb{E}_{z \sim q(z|x)} \left [ \log p(x) \right ] \\
  &amp; \ \ \ \ - \mathbb{E}_{z \sim q(z|x)} \left [ \log p(x|z)\right ] -  \mathbb{E}_{z \sim q(z|x)} \left [ \log p(z) \right ]
\end{split}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;This equation actually involves the target that we want to maximize, &lt;code class=&quot;highlighter-rouge&quot;&gt;$\mathbb{E}_{z \sim q(z|x)} \left [ \log p(x) \right ]$&lt;/code&gt;, thus we change the format of the above target as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{split}
  \mathbb{E}_{z \sim q(z|x)} \left [ \log p(x) \right ] - {\rm KL}(q(z|x)\|p(z|x)) &amp;= \mathbb{E}_{z \sim q(z|x)} \left [ \log p(x|z) \right ] + \mathbb{E}_{z \sim q(z|x)} \left [ \log p(z)  -  \log q(z|x) \right ] \\
   &amp;= \mathbb{E}_{z \sim q(z|x)} \left [ \log p(x|z) \right ] - {\rm KL}(q(z|x) \| p(z)).
\end{split}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;Because the KL divergence also great equal than 0, thus we can have the lower bound of &lt;code class=&quot;highlighter-rouge&quot;&gt;$\mathbb{E}_{z \sim q(z|x)} \left [ \log p(x) \right ]$&lt;/code&gt; as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{E}_{z \sim q(z|x)} \left [ \log p(x) \right ] \ge \mathbb{E}_{z \sim q(z|x)} \left [ \log p(x|z) \right ] - {\rm KL}(q(z|x) \| p(z)).&lt;/script&gt;

&lt;p&gt;And this is actually the training target function of VAE which includes two part, the first term is reconstruction loss &lt;code class=&quot;highlighter-rouge&quot;&gt;$\mathbb{E}_{z \sim q(z|x)} \left [ \log p(x|z) \right ]$&lt;/code&gt; which means giving a z sampled from &lt;code class=&quot;highlighter-rouge&quot;&gt;$ q(z|x)$&lt;/code&gt; and generating $x$ based on $z$. The second term is regularisation term which calculate the KL divergence between &lt;code class=&quot;highlighter-rouge&quot;&gt;q(z|x)&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;p(z)&lt;/code&gt; where the prior of $z$ is a simple gaussian distribution $p(z) = N(0, I)$.&lt;/p&gt;

&lt;p&gt;However, we find the whole process is not differentiable when we draw the computational graph, thus we can not directly use gradient decent to optimise the target. To address this problem, we use reparametrisation trick to sample $z$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;q(z|x) = N(\mu(x), \sigma(x)) = \mu(x) + \sigma(x) N(0,I).&lt;/script&gt;

&lt;p&gt;In addition to above way for the derivation of the VAE’s lower bound, we have another way to derive the lower bound of the VAE:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{split}
\log p(x) &amp;= \log \int_{z} p(x|z)p(z) = \log \int_{z} p(x|z)p(z) \frac{q(z|x)}{q(z|x)} \\
          &amp;= \log \mathbb{E}_{z \sim q(z|x)} \left [\frac{p(x|z)p(z)}{q(z|x)} \right ] \ \ \ {\rm applying \  Jensen’s \ inequality} \\
          &amp;\ge \mathbb{E}_{z \sim q(z|x)} \left [ \log \frac{p(x|z)p(z)}{q(z|x)} \right ].
\end{split}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;In general, we call this objective as evidence lower bound (ELBO).&lt;/p&gt;

&lt;h2 id=&quot;using-policy-gradient-theorem-to-solve-vae-amortized-variational-inference&quot;&gt;Using policy gradient theorem to solve VAE (amortized variational inference)&lt;/h2&gt;

&lt;p&gt;Based on the aforementioned discussion, the objective of policy gradient methods and VAE share similar high-level structure, which is given by:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
L(\theta, \phi) = \mathbb{E}_{x \sim p_{\phi}(x)}\left [ f_{\theta}(x) \right ],
\end{equation}&lt;/script&gt;

&lt;p&gt;where $p_{\phi}(x)$ is a distribution parametrised by $\phi$ and $f_{\theta}(x)$ is a cost function for $x$. The objective is to obtain optimal parameters that minimise the target function. Here $f_{\theta}(x)$ in policy gradient and VAE represents the value function and log likelihood, respectively. Obviously, we can use policy gradient to solve VAE.&lt;/p&gt;

&lt;p&gt;Let’s take a look at the ELBO:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{split}
\log p(x) &amp;\ge \mathbb{E}_{z \sim q(z|x)} \left [ \log \frac{p(x|z)p(z)}{q(z|x)} \right ] \\
&amp; = \mathbb{E}_{z \sim q(z|x)} \left [ \log p(x|z) + \log p(z) \right ] - \mathbb{E}_{z \sim q(z|x)} q(z|x) \\
&amp; = \mathbb{E}_{z \sim q(z|x)} \left [ \log p(x|z) + \log p(z) \right ] + \mathcal{H}(q(z|x)),
\end{split}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;where &lt;code class=&quot;highlighter-rouge&quot;&gt;$\mathcal{H}(q(z|x))$&lt;/code&gt; is the entropy of &lt;code class=&quot;highlighter-rouge&quot;&gt;$q(z|x)$&lt;/code&gt;. Our goal is to maximise this ELBO to obtain optimal parameters of &lt;code class=&quot;highlighter-rouge&quot;&gt;$p(x|z)$&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;$q(z|x)$&lt;/code&gt;, which are denotes as $\theta$ and $\phi$. We denote the ELBO as $L(\theta, \phi)$. To apply gradient assent to obtain optimal $\theta$ and $\phi$ we need $\nabla_{\theta}L(\theta, \phi)$ and $\nabla_{\phi}L(\theta, \phi)$. We can use Monte Carlo method to calculate the partial derivative&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla_{\theta}L(\theta, \phi) \approx \mathbb{E}_{z \sim q(z|x)} \left [ \nabla_{\theta}\log p(x|z) \right ]&lt;/script&gt;

&lt;p&gt;Calculating the $\nabla_{\phi}L(\theta, \phi)$ is difficult.  However, if we consider the &lt;code class=&quot;highlighter-rouge&quot;&gt;$q(x|z)$&lt;/code&gt; as policy, $x$ as state, $z$ as action, &lt;code class=&quot;highlighter-rouge&quot;&gt;$\log p(x|z)+\log p(z)$&lt;/code&gt; as reward function. The gradient of the entropy term can be directly calculated while we can use gradient theorem to calculate the term &lt;code class=&quot;highlighter-rouge&quot;&gt;$\mathbb{E}_{z \sim q(z|x)} \left [ \log p(x|z) + \log p(z) \right ]$&lt;/code&gt;, as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla_{\phi}\mathbb{E}_{z \sim q(z|x)} \left [ \log p(x|z) + \log p(z) \right ] = \mathbb{E}_{z \sim q(z|x)} \left [ \nabla_{\phi} \log q(z|x) \sum_{z \sim q(z|x)} r(z,x)  \right ]&lt;/script&gt;

&lt;p&gt;where &lt;code class=&quot;highlighter-rouge&quot;&gt;$r(z,x) = \log p(x|z) + \log p(z) $&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In summary, we have the partial derivative for both $\theta$ and $\phi$ as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{split}
\nabla_{\theta}L(\theta, \phi) &amp;\approx \mathbb{E}_{z \sim q(z|x)} \left [ \nabla_{\theta}\log p(x|z) \right ] \\

\nabla_{\phi}L(\theta, \phi) &amp;\approx \mathbb{E}_{z \sim q(z|x)} \left [ \nabla_{\phi} \log q(z|x) \sum_{z \sim q(z|x)} r(z,x)  \right ] + \nabla_{\phi} \mathcal{H}(q(z|x))
\end{split}
\end{equation} %]]&gt;&lt;/script&gt;</content><author><name>Jin Wang</name><email>jinwang1904@gmail.com</email></author><category term="reinforcement-learning" /><category term="VAE" /><summary type="html">Policy gradient theorem is the basis of many deep reinforcement learning methods (e.g., PPO, TRPO, and SAC). Meanwhile, Variational Auto-encoder (VAE) is one of the popular generative methods in machine learning. Since the target problems of these approaches share the same formulation, we can use policy gradient as an alternative training method for VAE.</summary></entry></feed>